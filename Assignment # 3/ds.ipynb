{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baebd0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Data from disease_features.csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import streamlit as st\n",
    "df = pd.read_csv('disease_features.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9ac3353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One‑hot encoding for Risk Factors: (25, 170)\n",
      "One‑hot encoding for Symptoms: (25, 189)\n",
      "One‑hot encoding for Signs: (25, 62)\n",
      "\n",
      "Sparsity Comparison:\n",
      "TF‑IDF Sparsity:  92.96%\n",
      "One‑hot Sparsity: 95.15%\n",
      "\n",
      "Dimensionality Comparison:\n",
      "TF‑IDF features:  1020\n",
      "One‑hot features: 421\n",
      "\n",
      "Matrix Statistics:\n",
      "TF‑IDF:\n",
      "- Non‑zero elements: 1795\n",
      "- Mean value:        0.0119\n",
      "\n",
      "One‑hot:\n",
      "- Non‑zero elements: 510\n",
      "- Mean value:        0.0485\n",
      "\n",
      "TF‑IDF Value Distribution:\n",
      "- Min:    0.0239\n",
      "- Max:    0.6903\n",
      "- Mean:   0.1687\n",
      "- Median: 0.1618\n",
      "- Std Dev:0.0743\n",
      "\n",
      "One‑hot Value Distribution:\n",
      "- Unique values: [0 1]\n",
      "- Min:           0.0000\n",
      "- Max:           1.0000\n",
      "\n",
      "Memory Usage:\n",
      "TF‑IDF:  14.02 KB\n",
      "One‑hot: 82.23 KB\n",
      "\n",
      "Information Density:\n",
      "TF‑IDF avg features per sample:  71.80\n",
      "One‑hot avg features per sample: 20.40\n"
     ]
    }
   ],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Task 1: TF‑IDF Feature Extraction and One‑Hot Encoding\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import ast\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Step 1: Parse string‑encoded lists into real Python lists\n",
    "columns_to_parse = ['Risk Factors', 'Symptoms', 'Signs', 'Subtypes']\n",
    "for col in columns_to_parse:\n",
    "    # If the cell is not null, convert the string \"[a, b]\" into a Python list ['a', 'b']\n",
    "    df[col] = df[col].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else [])\n",
    "\n",
    "# Step 2: Build text columns for TF‑IDF\n",
    "# Join each list into one space‑separated string, ready for text vectorization\n",
    "df['Risk Factors_str'] = df['Risk Factors'].apply(lambda lst: ' '.join(lst))\n",
    "df['Symptoms_str']     = df['Symptoms'].apply(lambda lst: ' '.join(lst))\n",
    "df['Signs_str']        = df['Signs'].apply(lambda lst: ' '.join(lst))\n",
    "\n",
    "# Step 3: Vectorize each text column with TF‑IDF\n",
    "vectorizers = {}    # To store each TfidfVectorizer object\n",
    "tfidf_matrices = {} # To store the resulting sparse matrices\n",
    "\n",
    "for col in ['Risk Factors_str', 'Symptoms_str', 'Signs_str']:\n",
    "    # Initialize a fresh TF‑IDF vectorizer\n",
    "    vec = TfidfVectorizer()\n",
    "    # Fit to the column and transform into a sparse matrix\n",
    "    mat = vec.fit_transform(df[col])\n",
    "    # Keep references for later use\n",
    "    vectorizers[col] = vec\n",
    "    tfidf_matrices[col] = mat\n",
    "    # Optional debug: print(f\"TF‑IDF on {col} → shape {mat.shape}\")\n",
    "\n",
    "# Step 4: Combine the three TF‑IDF matrices into one\n",
    "combined_tfidf = hstack([\n",
    "    tfidf_matrices['Risk Factors_str'],\n",
    "    tfidf_matrices['Symptoms_str'],\n",
    "    tfidf_matrices['Signs_str']\n",
    "])\n",
    "# Convert to dense array if you need to inspect values directly\n",
    "dense_matrix = combined_tfidf.toarray()\n",
    "\n",
    "# Step 5: One‑Hot encode the original list columns\n",
    "mlb = MultiLabelBinarizer()\n",
    "onehot_matrices = {}\n",
    "\n",
    "for col in ['Risk Factors', 'Symptoms', 'Signs']:\n",
    "    # Transform each list of labels into a binary indicator matrix\n",
    "    onehot = mlb.fit_transform(df[col])\n",
    "    onehot_matrices[col] = onehot\n",
    "    print(f\"One‑hot encoding for {col}: {onehot.shape}\")\n",
    "\n",
    "# Stack the one‑hot matrices side by side into one array\n",
    "combined_onehot = np.hstack([\n",
    "    onehot_matrices['Risk Factors'],\n",
    "    onehot_matrices['Symptoms'],\n",
    "    onehot_matrices['Signs']\n",
    "])\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Comparison Metrics\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1. Sparsity: fraction of zero entries in each matrix\n",
    "tfidf_sparsity  = 1 - (np.count_nonzero(dense_matrix) / dense_matrix.size)\n",
    "onehot_sparsity = 1 - (np.count_nonzero(combined_onehot) / combined_onehot.size)\n",
    "print(\"\\nSparsity Comparison:\")\n",
    "print(f\"TF‑IDF Sparsity:  {tfidf_sparsity:.2%}\")\n",
    "print(f\"One‑hot Sparsity: {onehot_sparsity:.2%}\")\n",
    "\n",
    "# 2. Dimensionality: number of features in each encoding\n",
    "print(\"\\nDimensionality Comparison:\")\n",
    "print(f\"TF‑IDF features:  {combined_tfidf.shape[1]}\")\n",
    "print(f\"One‑hot features: {combined_onehot.shape[1]}\")\n",
    "\n",
    "# 3. Basic statistics: density and mean values\n",
    "print(\"\\nMatrix Statistics:\")\n",
    "print(\"TF‑IDF:\")\n",
    "print(f\"- Non‑zero elements: {combined_tfidf.nnz}\")\n",
    "print(f\"- Mean value:        {combined_tfidf.mean():.4f}\")\n",
    "print(\"\\nOne‑hot:\")\n",
    "print(f\"- Non‑zero elements: {np.count_nonzero(combined_onehot)}\")\n",
    "print(f\"- Mean value:        {combined_onehot.mean():.4f}\")\n",
    "\n",
    "# 4. Feature distribution for TF‑IDF (non‑zero values only)\n",
    "tfidf_values = combined_tfidf.data\n",
    "print(\"\\nTF‑IDF Value Distribution:\")\n",
    "print(f\"- Min:    {tfidf_values.min():.4f}\")\n",
    "print(f\"- Max:    {tfidf_values.max():.4f}\")\n",
    "print(f\"- Mean:   {tfidf_values.mean():.4f}\")\n",
    "print(f\"- Median: {np.median(tfidf_values):.4f}\")\n",
    "print(f\"- Std Dev:{tfidf_values.std():.4f}\")\n",
    "\n",
    "# 5. One‑hot value distribution (only 0s and 1s)\n",
    "onehot_values = combined_onehot.flatten()\n",
    "print(\"\\nOne‑hot Value Distribution:\")\n",
    "print(f\"- Unique values: {np.unique(onehot_values)}\")\n",
    "print(f\"- Min:           {onehot_values.min():.4f}\")\n",
    "print(f\"- Max:           {onehot_values.max():.4f}\")\n",
    "\n",
    "# 6. Memory usage of the data arrays\n",
    "print(\"\\nMemory Usage:\")\n",
    "print(f\"TF‑IDF:  {combined_tfidf.data.nbytes  / 1024:.2f} KB\")\n",
    "print(f\"One‑hot: {combined_onehot.nbytes     / 1024:.2f} KB\")\n",
    "\n",
    "# 7. Information density: average non‑zero features per sample\n",
    "print(\"\\nInformation Density:\")\n",
    "print(f\"TF‑IDF avg features per sample:  {combined_tfidf.nnz / combined_tfidf.shape[0]:.2f}\")\n",
    "print(f\"One‑hot avg features per sample: {np.count_nonzero(combined_onehot) / combined_onehot.shape[0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc2f6091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratios:\n",
      "\n",
      "TF‑IDF (TruncatedSVD):\n",
      "Total variance explained: 0.1313\n",
      " Component 1: 0.0089\n",
      " Component 2: 0.0657\n",
      " Component 3: 0.0567\n",
      "\n",
      "One‑hot (PCA):\n",
      "Total variance explained: 0.2801\n",
      " Component 1: 0.1106\n",
      " Component 2: 0.0951\n",
      " Component 3: 0.0744\n"
     ]
    }
   ],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Task 2: Dimensionality Reduction\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Import necessary libraries for decomposition and plotting\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Apply dimensionality reduction methods\n",
    "\n",
    "# 1.1 Truncated SVD on TF‑IDF matrix\n",
    "#    - TruncatedSVD works directly with sparse matrices\n",
    "#    - We choose n_components = 3 to reduce to 3 latent dimensions\n",
    "n_components = 3\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "# Fit SVD to the combined TF‑IDF sparse matrix and transform\n",
    "tfidf_reduced = svd.fit_transform(combined_tfidf)\n",
    "\n",
    "# 1.2 PCA on one‑hot encoded matrix\n",
    "#    - PCA requires a dense array, but one‑hot is already dense\n",
    "pca = PCA(n_components=n_components)\n",
    "# Fit PCA to the combined one‑hot array and transform\n",
    "onehot_reduced = pca.fit_transform(combined_onehot)\n",
    "\n",
    "# 1.3 Print explained variance ratios for each component\n",
    "print(\"Explained Variance Ratios:\\n\")\n",
    "\n",
    "# For TruncatedSVD (TF‑IDF)\n",
    "print(\"TF‑IDF (TruncatedSVD):\")\n",
    "total_var_svd = svd.explained_variance_ratio_.sum()\n",
    "print(f\"Total variance explained: {total_var_svd:.4f}\")\n",
    "for idx, ratio in enumerate(svd.explained_variance_ratio_, start=1):\n",
    "    print(f\" Component {idx}: {ratio:.4f}\")\n",
    "\n",
    "# For PCA (One‑Hot)\n",
    "print(\"\\nOne‑hot (PCA):\")\n",
    "total_var_pca = pca.explained_variance_ratio_.sum()\n",
    "print(f\"Total variance explained: {total_var_pca:.4f}\")\n",
    "for idx, ratio in enumerate(pca.explained_variance_ratio_, start=1):\n",
    "    print(f\" Component {idx}: {ratio:.4f}\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Step 2: 2D Visualization of Reduced Dimensions\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 2.1 Map each disease to a clinical category for coloring\n",
    "category_mapping = {\n",
    "    \"Acute Coronary Syndrome\":        \"Cardiovascular\",\n",
    "    \"Adrenal Insufficiency\":           \"Endocrine\",\n",
    "    \"Alzheimer\":                       \"Neurological\",\n",
    "    \"Aortic Dissection\":               \"Cardiovascular\",\n",
    "    \"Asthma\":                          \"Respiratory\",\n",
    "    \"Atrial Fibrillation\":             \"Cardiovascular\",\n",
    "    \"Cardiomyopathy\":                  \"Cardiovascular\",\n",
    "    \"COPD\":                            \"Respiratory\",\n",
    "    \"Diabetes\":                        \"Endocrine\",\n",
    "    \"Epilepsy\":                        \"Neurological\",\n",
    "    \"Gastritis\":                       \"Gastrointestinal\",\n",
    "    \"Gastro-oesophageal Reflux Disease\":\"Gastrointestinal\",\n",
    "    \"Heart Failure\":                   \"Cardiovascular\",\n",
    "    \"Hyperlipidemia\":                  \"Cardiovascular\",\n",
    "    \"Hypertension\":                    \"Cardiovascular\",\n",
    "    \"Migraine\":                        \"Neurological\",\n",
    "    \"Multiple Sclerosis\":              \"Neurological\",\n",
    "    \"Peptic Ulcer Disease\":            \"Gastrointestinal\",\n",
    "    \"Pituitary Disease\":               \"Endocrine\",\n",
    "    \"Pneumonia\":                       \"Respiratory\",\n",
    "    \"Pulmonary Embolism\":              \"Cardiovascular\",\n",
    "    \"Stroke\":                          \"Neurological\",\n",
    "    \"Thyroid Disease\":                 \"Endocrine\",\n",
    "    \"Tuberculosis\":                    \"Infectious\",\n",
    "    \"Upper Gastrointestinal Bleeding\": \"Gastrointestinal\"\n",
    "}\n",
    "\n",
    "# Add a 'Category' column to df by mapping disease names\n",
    "df['Category'] = df['Disease'].map(category_mapping)\n",
    "\n",
    "# Convert categories to numeric codes for color mapping\n",
    "unique_categories = df['Category'].unique()\n",
    "category_to_num = {cat: idx for idx, cat in enumerate(unique_categories)}\n",
    "category_nums = df['Category'].map(category_to_num)\n",
    "\n",
    "# 2.2 Create side‑by‑side 2D scatter plots\n",
    "plt.figure(figsize=(16, 7))\n",
    "\n",
    "# Plot for TF‑IDF reduced data\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(\n",
    "    tfidf_reduced[:, 0], tfidf_reduced[:, 1],\n",
    "    c=category_nums, cmap='viridis', alpha=0.8\n",
    ")\n",
    "plt.title('TF‑IDF Vectorization (2D)')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "\n",
    "# Build a custom legend using Line2D handles\n",
    "from matplotlib.lines import Line2D\n",
    "legend_handles = [\n",
    "    Line2D([0], [0], marker='o', color='w',\n",
    "           markerfacecolor=plt.cm.viridis(category_to_num[cat]/len(category_to_num)),\n",
    "           markersize=8, label=cat)\n",
    "    for cat in unique_categories\n",
    "]\n",
    "plt.legend(handles=legend_handles, title=\"Disease Categories\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot for One‑Hot reduced data\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(\n",
    "    onehot_reduced[:, 0], onehot_reduced[:, 1],\n",
    "    c=category_nums, cmap='viridis', alpha=0.8\n",
    ")\n",
    "plt.title('One‑Hot Encoding (2D)')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.legend(handles=legend_handles, title=\"Disease Categories\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Optional: 3D Visualization (if n_components == 3)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "if n_components == 3:\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 7))\n",
    "\n",
    "    # 3D scatter for TF‑IDF\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    ax1.scatter(\n",
    "        tfidf_reduced[:, 0], tfidf_reduced[:, 1], tfidf_reduced[:, 2],\n",
    "        c=category_nums, cmap='viridis', alpha=0.8\n",
    "    )\n",
    "    ax1.set_title('TF‑IDF Vectorization (3D)')\n",
    "    ax1.set_xlabel('Component 1')\n",
    "    ax1.set_ylabel('Component 2')\n",
    "    ax1.set_zlabel('Component 3')\n",
    "    ax1.legend(handles=legend_handles, title=\"Disease Categories\")\n",
    "\n",
    "    # 3D scatter for One‑Hot\n",
    "    ax2 = fig.add_subplot(122, projection='3d')\n",
    "    ax2.scatter(\n",
    "        onehot_reduced[:, 0], onehot_reduced[:, 1], onehot_reduced[:, 2],\n",
    "        c=category_nums, cmap='viridis', alpha=0.8\n",
    "    )\n",
    "    ax2.set_title('One‑Hot Encoding (3D)')\n",
    "    ax2.set_xlabel('Component 1')\n",
    "    ax2.set_ylabel('Component 2')\n",
    "    ax2.set_zlabel('Component 3')\n",
    "    ax2.legend(handles=legend_handles, title=\"Disease Categories\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Discussion (as comments):\n",
    "#\n",
    "# - TF‑IDF clusters appear more distinct, indicating that weighted term frequencies\n",
    "#   help separate disease categories in lower-dimensional space.\n",
    "# - One‑Hot clusters overlap more, since all features are equally weighted.\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab21d157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. KNN Model Comparison by Normalization Method:\n",
      "\n",
      "--- KNN with None Normalization ---\n",
      "            Accuracy         F1-Score        \n",
      "Feature       TF-IDF One-hot   TF-IDF One-hot\n",
      "k Metric                                     \n",
      "3 cosine      0.6806  0.5972   0.6362  0.5574\n",
      "  euclidean   0.4861  0.2731   0.3804  0.1616\n",
      "  manhattan   0.3194  0.2731   0.1608  0.1616\n",
      "5 cosine      0.7222  0.3981   0.6778  0.3178\n",
      "  euclidean   0.4815  0.2731   0.3343  0.1505\n",
      "  manhattan   0.3611  0.2731   0.2163  0.1505\n",
      "7 cosine      0.5972  0.4444   0.5197  0.3986\n",
      "  euclidean   0.3981  0.2778   0.2509  0.1545\n",
      "  manhattan   0.3194  0.2778   0.1571  0.1545\n",
      "\n",
      "--- KNN with StandardScaler Normalization ---\n",
      "            Accuracy         F1-Score        \n",
      "Feature       TF-IDF One-hot   TF-IDF One-hot\n",
      "k Metric                                     \n",
      "3 cosine      0.6806  0.5139   0.6245  0.4730\n",
      "  euclidean   0.3194  0.2731   0.1571  0.1590\n",
      "  manhattan   0.3194  0.2731   0.1571  0.1590\n",
      "5 cosine      0.6019  0.3565   0.5111  0.2405\n",
      "  euclidean   0.3194  0.2361   0.1571  0.1167\n",
      "  manhattan   0.3194  0.2731   0.1571  0.1493\n",
      "7 cosine      0.4028  0.3981   0.2747  0.3185\n",
      "  euclidean   0.3194  0.2361   0.1571  0.1432\n",
      "  manhattan   0.3194  0.2407   0.1571  0.1378\n",
      "\n",
      "--- KNN with MinMaxScaler Normalization ---\n",
      "            Accuracy         F1-Score        \n",
      "Feature       TF-IDF One-hot   TF-IDF One-hot\n",
      "k Metric                                     \n",
      "3 cosine      0.6806  0.5556   0.6428  0.5296\n",
      "  euclidean   0.3611  0.2731   0.2056  0.1616\n",
      "  manhattan   0.3194  0.2731   0.1571  0.1616\n",
      "5 cosine      0.6435  0.4398   0.5755  0.3636\n",
      "  euclidean   0.3194  0.3148   0.1571  0.1748\n",
      "  manhattan   0.3194  0.3148   0.1571  0.1748\n",
      "7 cosine      0.4861  0.4028   0.3981  0.3292\n",
      "  euclidean   0.3194  0.2778   0.1571  0.1545\n",
      "  manhattan   0.3194  0.2778   0.1571  0.1545\n",
      "\n",
      "2. Logistic Regression Model Comparison by Normalization Method:\n",
      "               Accuracy         F1-Score         Precision          Recall  \\\n",
      "Feature         One-hot  TF-IDF  One-hot  TF-IDF   One-hot  TF-IDF One-hot   \n",
      "Normalization                                                                \n",
      "MinMaxScaler     0.4028  0.5185   0.3132  0.4123    0.2887  0.4117  0.4028   \n",
      "None             0.4028  0.5602   0.3132  0.4783    0.2887  0.5075  0.4028   \n",
      "StandardScaler   0.3981  0.7222   0.3747  0.6750    0.4537  0.7032  0.3981   \n",
      "\n",
      "                        \n",
      "Feature         TF-IDF  \n",
      "Normalization           \n",
      "MinMaxScaler    0.5185  \n",
      "None            0.5602  \n",
      "StandardScaler  0.7222  \n",
      "\n",
      "3. Best Models by Feature Type:\n",
      "                   Model Normalization  k  Metric Accuracy Precision  Recall  \\\n",
      "Best TF-IDF Model    KNN          None  5  cosine   0.7222    0.6840  0.7222   \n",
      "Best One-hot Model   KNN          None  3  cosine   0.5972    0.5887  0.5972   \n",
      "\n",
      "                   F1-Score  \n",
      "Best TF-IDF Model    0.6778  \n",
      "Best One-hot Model   0.5574  \n",
      "\n",
      "4. Top 5 Models Overall:\n",
      "                 Model Feature   Normalization    k  Metric Accuracy  \\\n",
      "1                  KNN  TF-IDF            None    5  cosine   0.7222   \n",
      "2  Logistic Regression  TF-IDF  StandardScaler  N/A     N/A   0.7222   \n",
      "3                  KNN  TF-IDF    MinMaxScaler    3  cosine   0.6806   \n",
      "4                  KNN  TF-IDF            None    3  cosine   0.6806   \n",
      "5                  KNN  TF-IDF  StandardScaler    3  cosine   0.6806   \n",
      "\n",
      "  Precision  Recall F1-Score  \n",
      "1    0.6840  0.7222   0.6778  \n",
      "2    0.7032  0.7222   0.6750  \n",
      "3    0.6961  0.6806   0.6428  \n",
      "4    0.6609  0.6806   0.6362  \n",
      "5    0.6498  0.6806   0.6245  \n",
      "\n",
      "5. Effect of k Value on KNN Performance:\n",
      "Using None normalization and cosine metric:\n",
      "        Accuracy         F1-Score        \n",
      "Feature  One-hot  TF-IDF  One-hot  TF-IDF\n",
      "k                                        \n",
      "3         0.5972  0.6806   0.5574  0.6362\n",
      "5         0.3981  0.7222   0.3178  0.6778\n",
      "7         0.4444  0.5972   0.3986  0.5197\n"
     ]
    }
   ],
   "source": [
    "#Task 3: Train KNN Models and Logistic Regression\n",
    "\n",
    "#Step 1 #########################################################################################\n",
    "# Prepare for KNN modeling with different k values and distance metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler  # Added for normalization\n",
    "from sklearn.pipeline import Pipeline  # Added for creating pipelines with normalization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Suppress warnings\n",
    "\n",
    "# Define the target variable (disease categories)\n",
    "target = df['Category']\n",
    "\n",
    "# Define k values and distance metrics to test\n",
    "k_values = [3, 5, 7]\n",
    "metrics = ['euclidean', 'manhattan', 'cosine']\n",
    "\n",
    "# Define scoring metrics for cross-validation\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score, average='weighted', zero_division=0),\n",
    "    'recall': make_scorer(recall_score, average='weighted', zero_division=0),\n",
    "    'f1': make_scorer(f1_score, average='weighted', zero_division=0)\n",
    "}\n",
    "\n",
    "# Create DataFrames to store results\n",
    "results_df = pd.DataFrame(columns=['Model', 'Feature', 'Normalization', 'k', 'Metric', 'Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "\n",
    "#Step 2 #########################################################################################\n",
    "# Perform 3-fold cross-validation for KNN with different configurations\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Prepare data\n",
    "tfidf_array = combined_tfidf.toarray()\n",
    "onehot_array = combined_onehot\n",
    "\n",
    "# Define normalization methods to test\n",
    "normalizers = {\n",
    "    'None': None,\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler()\n",
    "}\n",
    "\n",
    "# For TF-IDF features\n",
    "for norm_name, normalizer in normalizers.items():\n",
    "    for k in k_values:\n",
    "        for metric in metrics:\n",
    "            try:\n",
    "                if normalizer is None:\n",
    "                    # No normalization\n",
    "                    knn = KNeighborsClassifier(n_neighbors=k, metric=metric, weights='distance')\n",
    "                    cv_results = cross_validate(knn, tfidf_array, target, cv=cv, scoring=scoring)\n",
    "                else:\n",
    "                    # With normalization using pipeline\n",
    "                    pipeline = Pipeline([\n",
    "                        ('normalizer', normalizer),\n",
    "                        ('knn', KNeighborsClassifier(n_neighbors=k, metric=metric, weights='distance'))\n",
    "                    ])\n",
    "                    cv_results = cross_validate(pipeline, tfidf_array, target, cv=cv, scoring=scoring)\n",
    "                \n",
    "                # Store results\n",
    "                new_row = pd.DataFrame([{\n",
    "                    'Model': 'KNN',\n",
    "                    'Feature': 'TF-IDF',\n",
    "                    'Normalization': norm_name,\n",
    "                    'k': k,\n",
    "                    'Metric': metric,\n",
    "                    'Accuracy': cv_results['test_accuracy'].mean(),\n",
    "                    'Precision': cv_results['test_precision'].mean(),\n",
    "                    'Recall': cv_results['test_recall'].mean(),\n",
    "                    'F1-Score': cv_results['test_f1'].mean()\n",
    "                }])\n",
    "                results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "            except Exception as e:\n",
    "                # Handle any errors\n",
    "                new_row = pd.DataFrame([{\n",
    "                    'Model': 'KNN',\n",
    "                    'Feature': 'TF-IDF',\n",
    "                    'Normalization': norm_name,\n",
    "                    'k': k,\n",
    "                    'Metric': metric,\n",
    "                    'Accuracy': np.nan,\n",
    "                    'Precision': np.nan,\n",
    "                    'Recall': np.nan,\n",
    "                    'F1-Score': np.nan\n",
    "                }])\n",
    "                results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "# For One-hot encoded features\n",
    "for norm_name, normalizer in normalizers.items():\n",
    "    for k in k_values:\n",
    "        for metric in metrics:\n",
    "            try:\n",
    "                if normalizer is None:\n",
    "                    # No normalization\n",
    "                    knn = KNeighborsClassifier(n_neighbors=k, metric=metric, weights='distance')\n",
    "                    cv_results = cross_validate(knn, onehot_array, target, cv=cv, scoring=scoring)\n",
    "                else:\n",
    "                    # With normalization using pipeline\n",
    "                    pipeline = Pipeline([\n",
    "                        ('normalizer', normalizer),\n",
    "                        ('knn', KNeighborsClassifier(n_neighbors=k, metric=metric, weights='distance'))\n",
    "                    ])\n",
    "                    cv_results = cross_validate(pipeline, onehot_array, target, cv=cv, scoring=scoring)\n",
    "                \n",
    "                # Store results\n",
    "                new_row = pd.DataFrame([{\n",
    "                    'Model': 'KNN',\n",
    "                    'Feature': 'One-hot',\n",
    "                    'Normalization': norm_name,\n",
    "                    'k': k,\n",
    "                    'Metric': metric,\n",
    "                    'Accuracy': cv_results['test_accuracy'].mean(),\n",
    "                    'Precision': cv_results['test_precision'].mean(),\n",
    "                    'Recall': cv_results['test_recall'].mean(),\n",
    "                    'F1-Score': cv_results['test_f1'].mean()\n",
    "                }])\n",
    "                results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "            except Exception as e:\n",
    "                # Handle any errors\n",
    "                new_row = pd.DataFrame([{\n",
    "                    'Model': 'KNN',\n",
    "                    'Feature': 'One-hot',\n",
    "                    'Normalization': norm_name,\n",
    "                    'k': k,\n",
    "                    'Metric': metric,\n",
    "                    'Accuracy': np.nan,\n",
    "                    'Precision': np.nan,\n",
    "                    'Recall': np.nan,\n",
    "                    'F1-Score': np.nan\n",
    "                }])\n",
    "                results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "#Step 3 #########################################################################################\n",
    "# Train Logistic Regression models on both matrices with and without normalization\n",
    "for norm_name, normalizer in normalizers.items():\n",
    "    # For TF-IDF features\n",
    "    try:\n",
    "        if normalizer is None:\n",
    "            # No normalization\n",
    "            lr = LogisticRegression(max_iter=2000, solver='saga', multi_class='auto', class_weight='balanced')\n",
    "            lr_results = cross_validate(lr, tfidf_array, target, cv=cv, scoring=scoring)\n",
    "        else:\n",
    "            # With normalization using pipeline\n",
    "            pipeline = Pipeline([\n",
    "                ('normalizer', normalizer),\n",
    "                ('lr', LogisticRegression(max_iter=2000, solver='saga', multi_class='auto', class_weight='balanced'))\n",
    "            ])\n",
    "            lr_results = cross_validate(pipeline, tfidf_array, target, cv=cv, scoring=scoring)\n",
    "        \n",
    "        # Store results\n",
    "        new_row = pd.DataFrame([{\n",
    "            'Model': 'Logistic Regression',\n",
    "            'Feature': 'TF-IDF',\n",
    "            'Normalization': norm_name,\n",
    "            'k': 'N/A',\n",
    "            'Metric': 'N/A',\n",
    "            'Accuracy': lr_results['test_accuracy'].mean(),\n",
    "            'Precision': lr_results['test_precision'].mean(),\n",
    "            'Recall': lr_results['test_recall'].mean(),\n",
    "            'F1-Score': lr_results['test_f1'].mean()\n",
    "        }])\n",
    "        results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        # Handle any errors\n",
    "        new_row = pd.DataFrame([{\n",
    "            'Model': 'Logistic Regression',\n",
    "            'Feature': 'TF-IDF',\n",
    "            'Normalization': norm_name,\n",
    "            'k': 'N/A',\n",
    "            'Metric': 'N/A',\n",
    "            'Accuracy': np.nan,\n",
    "            'Precision': np.nan,\n",
    "            'Recall': np.nan,\n",
    "            'F1-Score': np.nan\n",
    "        }])\n",
    "        results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "    # For One-hot encoded features\n",
    "    try:\n",
    "        if normalizer is None:\n",
    "            # No normalization\n",
    "            lr = LogisticRegression(max_iter=2000, solver='saga', multi_class='auto', class_weight='balanced')\n",
    "            lr_results = cross_validate(lr, onehot_array, target, cv=cv, scoring=scoring)\n",
    "        else:\n",
    "            # With normalization using pipeline\n",
    "            pipeline = Pipeline([\n",
    "                ('normalizer', normalizer),\n",
    "                ('lr', LogisticRegression(max_iter=2000, solver='saga', multi_class='auto', class_weight='balanced'))\n",
    "            ])\n",
    "            lr_results = cross_validate(pipeline, onehot_array, target, cv=cv, scoring=scoring)\n",
    "        \n",
    "        # Store results\n",
    "        new_row = pd.DataFrame([{\n",
    "            'Model': 'Logistic Regression',\n",
    "            'Feature': 'One-hot',\n",
    "            'Normalization': norm_name,\n",
    "            'k': 'N/A',\n",
    "            'Metric': 'N/A',\n",
    "            'Accuracy': lr_results['test_accuracy'].mean(),\n",
    "            'Precision': lr_results['test_precision'].mean(),\n",
    "            'Recall': lr_results['test_recall'].mean(),\n",
    "            'F1-Score': lr_results['test_f1'].mean()\n",
    "        }])\n",
    "        results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        # Handle any errors\n",
    "        new_row = pd.DataFrame([{\n",
    "            'Model': 'Logistic Regression',\n",
    "            'Feature': 'One-hot',\n",
    "            'Normalization': norm_name,\n",
    "            'k': 'N/A',\n",
    "            'Metric': 'N/A',\n",
    "            'Accuracy': np.nan,\n",
    "            'Precision': np.nan,\n",
    "            'Recall': np.nan,\n",
    "            'F1-Score': np.nan\n",
    "        }])\n",
    "        results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "#Step 4 #########################################################################################\n",
    "# Display results in smaller, more focused tables\n",
    "# Format numeric columns to 4 decimal places\n",
    "numeric_cols = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "for col in numeric_cols:\n",
    "    results_df[col] = results_df[col].apply(lambda x: f\"{x:.4f}\" if not pd.isna(x) else \"N/A\")\n",
    "\n",
    "# 1. Compare KNN with different normalizations\n",
    "print(\"\\n1. KNN Model Comparison by Normalization Method:\")\n",
    "for norm_name in results_df['Normalization'].unique():\n",
    "    print(f\"\\n--- KNN with {norm_name} Normalization ---\")\n",
    "    \n",
    "    # Filter data for KNN with this normalization\n",
    "    knn_norm_data = results_df[(results_df['Model'] == 'KNN') & \n",
    "                              (results_df['Normalization'] == norm_name)]\n",
    "    \n",
    "    # Create a pivot table to compare TF-IDF vs One-hot\n",
    "    pivot_table = pd.pivot_table(\n",
    "        knn_norm_data,\n",
    "        values=['Accuracy', 'F1-Score'],\n",
    "        index=['k', 'Metric'],\n",
    "        columns=['Feature'],\n",
    "        aggfunc='first'  # Just take the first value since there should be only one\n",
    "    )\n",
    "    \n",
    "    # Reorder columns for better readability\n",
    "    if ('Accuracy', 'TF-IDF') in pivot_table.columns and ('Accuracy', 'One-hot') in pivot_table.columns:\n",
    "        pivot_table = pivot_table[[('Accuracy', 'TF-IDF'), ('Accuracy', 'One-hot'), \n",
    "                                  ('F1-Score', 'TF-IDF'), ('F1-Score', 'One-hot')]]\n",
    "    \n",
    "    # Display the table\n",
    "    print(pivot_table)\n",
    "\n",
    "# 2. Compare Logistic Regression with different normalizations\n",
    "print(\"\\n2. Logistic Regression Model Comparison by Normalization Method:\")\n",
    "lr_data = results_df[results_df['Model'] == 'Logistic Regression']\n",
    "\n",
    "# Create a pivot table to compare TF-IDF vs One-hot across normalizations\n",
    "lr_pivot = pd.pivot_table(\n",
    "    lr_data,\n",
    "    values=['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "    index=['Normalization'],\n",
    "    columns=['Feature'],\n",
    "    aggfunc='first'  # Just take the first value since there should be only one\n",
    ")\n",
    "\n",
    "# Display the table\n",
    "print(lr_pivot)\n",
    "\n",
    "# 3. Best Models by Feature Type\n",
    "print(\"\\n3. Best Models by Feature Type:\")\n",
    "# Convert F1-Score to float for finding the best models\n",
    "results_df['F1-Score_float'] = results_df['F1-Score'].apply(lambda x: float(x) if x != \"N/A\" else 0)\n",
    "\n",
    "# Find best model for TF-IDF\n",
    "best_tfidf = results_df[results_df['Feature'] == 'TF-IDF'].loc[results_df[results_df['Feature'] == 'TF-IDF']['F1-Score_float'].idxmax()]\n",
    "best_onehot = results_df[results_df['Feature'] == 'One-hot'].loc[results_df[results_df['Feature'] == 'One-hot']['F1-Score_float'].idxmax()]\n",
    "\n",
    "# Create a DataFrame with the best models\n",
    "best_models = pd.DataFrame([best_tfidf, best_onehot])\n",
    "best_models = best_models.drop('F1-Score_float', axis=1)\n",
    "best_models.index = ['Best TF-IDF Model', 'Best One-hot Model']\n",
    "\n",
    "# Display the table\n",
    "print(best_models[['Model', 'Normalization', 'k', 'Metric', 'Accuracy', 'Precision', 'Recall', 'F1-Score']])\n",
    "\n",
    "# 4. Top 5 Models Overall\n",
    "print(\"\\n4. Top 5 Models Overall:\")\n",
    "top_models = results_df.sort_values(by='F1-Score_float', ascending=False).head(5)\n",
    "top_models = top_models.drop('F1-Score_float', axis=1)\n",
    "top_models.index = range(1, len(top_models) + 1)  # Reset index to start from 1\n",
    "print(top_models[['Model', 'Feature', 'Normalization', 'k', 'Metric', 'Accuracy', 'Precision', 'Recall', 'F1-Score']])\n",
    "\n",
    "# 5. Effect of k Value on KNN Performance (Best Normalization and Metric)\n",
    "print(\"\\n5. Effect of k Value on KNN Performance:\")\n",
    "# Get the best normalization and metric from the top model\n",
    "best_norm = top_models[top_models['Model'] == 'KNN']['Normalization'].iloc[0] if not top_models[top_models['Model'] == 'KNN'].empty else results_df['Normalization'].iloc[0]\n",
    "best_metric = top_models[top_models['Model'] == 'KNN']['Metric'].iloc[0] if not top_models[top_models['Model'] == 'KNN'].empty else results_df['Metric'].iloc[0]\n",
    "\n",
    "# Filter data for the best normalization and metric\n",
    "k_effect_data = results_df[(results_df['Model'] == 'KNN') & \n",
    "                          (results_df['Normalization'] == best_norm) &\n",
    "                          (results_df['Metric'] == best_metric)]\n",
    "\n",
    "# Create a pivot table to compare k values\n",
    "k_pivot = pd.pivot_table(\n",
    "    k_effect_data,\n",
    "    values=['Accuracy', 'F1-Score'],\n",
    "    index=['k'],\n",
    "    columns=['Feature'],\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "# Display the table\n",
    "print(f\"Using {best_norm} normalization and {best_metric} metric:\")\n",
    "print(k_pivot)\n",
    "\n",
    "# Clean up temporary column\n",
    "results_df = results_df.drop('F1-Score_float', axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
